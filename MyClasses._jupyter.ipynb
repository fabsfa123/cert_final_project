{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9ec305",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc71ef6",
   "metadata": {},
   "source": [
    "Firstly, the required packages are imported and the configuration parameters are set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aedf137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.2.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.2.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pn        # data analysis and scientific computing\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "import cufflinks as cf     # charting - productivity tool for ploty and pandas \n",
    "cf.set_config_file(offline=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "plt.style.use('seaborn')  \n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "import pylab \n",
    "\n",
    "%config Completer.use_jedi = False #deactivates jedi to allow intellisense to run faster\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pn.errors.PerformanceWarning) # ignore performance warning\n",
    "\n",
    "import random             # data science packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Dropout\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from boruta import BorutaPy # automated variable selection\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import talib\n",
    "\n",
    "import itertools\n",
    "import winsound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2e3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instrument(object):\n",
    "    '''\n",
    "    Financial Instrument class\n",
    "    \n",
    "    Attributes:\n",
    "            Inputs:\n",
    "                file_name: str\n",
    "                    full name of the txt file containing OHLCV time series to be backtested including extension\n",
    "                    (.txt extension is required) \n",
    "                ticker: str\n",
    "                    ticker's name\n",
    "                description: str\n",
    "                    description of intrument\n",
    "            Data:\n",
    "                data: DataFrame\n",
    "                    raw OHLCV data\n",
    "                resampled_data: DataFrame\n",
    "                    resampled data\n",
    "    Methods:\n",
    "        get_data:\n",
    "            imports data\n",
    "        set_frequency:\n",
    "            changes frequency of time series\n",
    "        plot_daily_close:\n",
    "            plot time series of close price\n",
    "        CreateDescriptivePlots:\n",
    "            show 4 plots for descriptive statistics\n",
    "    '''\n",
    "    def __init__(self,file_name,ticker,description):\n",
    "        self.file_name = file_name\n",
    "        self.ticker = ticker\n",
    "        self.description = description\n",
    "        self.get_data()\n",
    "    \n",
    "    def set_frequency(self,f):\n",
    "        \"\"\"\n",
    "        Given a dataframe containing OHLCV intraday price data, it returns a Dataframe with a specified \n",
    "        frequency. \n",
    "        -------------------------------------------------------------------------------------\n",
    "        -------------------------------------------------------------------------------------\n",
    "    \n",
    "        Inputs:\n",
    "        -------\n",
    "        f: string\n",
    "            frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        d = self.data\n",
    "        \n",
    "        out = pn.DataFrame()\n",
    "        out['High']     = d.High.resample(f).max()\n",
    "        out['Low']      = d.Low.resample(f).min()\n",
    "        out['Open']     = d.Open.resample(f).first()\n",
    "        out['Close']    = d.Close.resample(f).last()\n",
    "        out['Volume']   = d.Volume.resample(f).sum()\n",
    "        out.dropna(inplace=True) \n",
    "        \n",
    "        self.resampled_data = out\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Import OHLCV time series\n",
    "        '''\n",
    "        _names= ['DateTime', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        self.data = pn.read_csv(self.file_name,delimiter=',',names= _names, \n",
    "                    index_col=0, parse_dates=True,infer_datetime_format=True)\n",
    "        print (self.data.info())\n",
    "    \n",
    "    def plot_daily_close(self, second_instrument = None):\n",
    "        \"\"\"\n",
    "        plot daily time series of close\n",
    "        \n",
    "        Inputs:\n",
    "        -------\n",
    "        \n",
    "        second_instrument: Instrument Class\n",
    "            use to add a new close line to the same plot\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        series1 = self.data.Close.resample('1d').last()\n",
    "        \n",
    "        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "        \n",
    "        trace1 = go.Scatter(x=series1.index, y=series1, mode='lines', name=self.ticker, yaxis='y')\n",
    "        fig.add_trace(trace1)\n",
    "        \n",
    "        # Add the second line to the secondary y-axis\n",
    "        if second_instrument is not None:\n",
    "            series2 = second_instrument.data.Close.resample('1d').last()\n",
    "            trace2 = go.Scatter(x=series2.index, y=series2, mode='lines', name=second_instrument.ticker, yaxis='y2')\n",
    "            fig.add_trace(trace2)\n",
    "            fig.update_layout(\n",
    "                title = 'Historical Quotes',\n",
    "                xaxis_title='Date',\n",
    "                yaxis_title= self.ticker,\n",
    "                yaxis2_title= second_instrument.ticker\n",
    "                )\n",
    "        else:\n",
    "            fig.update_layout(\n",
    "                title = self.description,\n",
    "                xaxis_title='Date',\n",
    "                yaxis_title= self.ticker,\n",
    "                )\n",
    "\n",
    "        # Display the plot\n",
    "        fig.show()\n",
    "    \n",
    "    def CreateDescriptivePlots(self,d=None):\n",
    "        \"\"\"\n",
    "        Creates 4 plots: histogram, box plot, qqplot, time series plot \n",
    "        -------------------------------------------------------------------------------------\n",
    "        -------------------------------------------------------------------------------------\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "        d: timeseries\n",
    "        title: string    \n",
    "        \"\"\" \n",
    "        \n",
    "        if d is None:\n",
    "            d = np.log(self.data['Close'] / self.data['Close'].shift(1))\n",
    "        else:\n",
    "            d = np.log(d['Close'] / d['Close'].shift(1))\n",
    "        \n",
    "        plt.figure(figsize=(18, 12))\n",
    "\n",
    "        plt.subplot(221)\n",
    "        plt.hist(d, bins=25, density=True, label=self.description)  \n",
    "        plt.xlabel(self.description)\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Histogram')\n",
    "\n",
    "        plt.subplot(222)\n",
    "        dd = pn.DataFrame()\n",
    "        dd[self.description] = d\n",
    "        dd.boxplot(self.description) \n",
    "        plt.xlabel('data set')\n",
    "        plt.ylabel('value')\n",
    "        plt.title('Boxplot');\n",
    "\n",
    "        plt.subplot(223)\n",
    "        stats.probplot(d, dist=\"norm\", plot=pylab)\n",
    "\n",
    "        plt.subplot(224)\n",
    "        plt.plot(d, lw=1.5, label=self.description)\n",
    "        plt.legend(loc=0)\n",
    "        plt.xlabel('time')\n",
    "        plt.ylabel('value')\n",
    "        plt.title('1st Data Set')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce6407ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyData(Instrument):\n",
    "    '''\n",
    "    Class containing methods and data containing all necessary data used for ML calibration, application and \n",
    "    strategy backtesting\n",
    "    \n",
    "    Attributes:\n",
    "            inputs:\n",
    "            -------------------------\n",
    "            -------------------------\n",
    "            data_name: str\n",
    "                data description\n",
    "            inst: Intrument object\n",
    "                instrument to be backtested\n",
    "            freq: str\n",
    "                frequency of data\n",
    "            vix: Intrument object\n",
    "                object containing vix data - if None, vix is not added as a predictor\n",
    "            \n",
    "            data:\n",
    "            ------------------------\n",
    "            ------------------------\n",
    "            cleansed_data: DataFrame\n",
    "                data aggregated at a given frequency, with vix added as a predictor, \n",
    "                and adjusted for trading hours and missing values\n",
    "            features_data: DataFrame:\n",
    "                cleansed data with features\n",
    "            labels_data: DataFrame\n",
    "                cleansed data with Labels\n",
    "            labels_train, labels_test, features_train,features_test: DataFrames\n",
    "                labels and features split in training and test. fetures are standardised\n",
    "                \n",
    "        Methods:\n",
    "            fill_OHLCV_missing_data: \n",
    "                fills missing values in a OHLCV DataFrame\n",
    "            add_vix:\n",
    "                merge ViX OHLCV data with main instrument to be predicted\n",
    "            set_frequency:\n",
    "                resample dataframe with one or two OHLCV instruments \n",
    "            number_of_bars_per_day:\n",
    "                returns max number of OHLCV bars per day in a DataFrame\n",
    "            create_features:\n",
    "                adds multiple features to a DataFrame for intraday ML forecasting\n",
    "            create_labels:\n",
    "                create ML labels\n",
    "            split_train_test:\n",
    "                split features and labels into train and test\n",
    "            standardise:\n",
    "                standardise ML data\n",
    "            split_data_standardise:\n",
    "                split features and labels into training and test and then standardise features\n",
    "            set_trading_hours:\n",
    "                delete bars outside of a specified time span to align ML calibration with application of trading\n",
    "                strategy\n",
    "            \n",
    "    '''\n",
    "    def __init__(self,inst,freq,data_name='',vix=None):\n",
    "        self.data_name = data_name\n",
    "        self.inst = inst\n",
    "        self.freq = freq\n",
    "        self.vix = vix\n",
    "        self.add_vix()\n",
    "        self.set_frequency()\n",
    "    \n",
    "    def fill_OHLCV_missing_data(self,d,ticker):\n",
    "        '''\n",
    "        fills missing values in a OHLCV DataFrame\n",
    "        \n",
    "        Inputs:\n",
    "            d: DataFrame\n",
    "            ticker: str\n",
    "        Output: DataFrame\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        d['Volume_'+ticker]=d['Volume_'+ticker].fillna(0.)         # fill missing volume with 0 \n",
    "        d['Close_'+ticker].fillna(method='ffill',inplace=True)     # forward fill Close\n",
    "        d['Open_'+ticker].fillna(d['Close_'+ticker],inplace=True)  # fill missing Open with forward filled Close\n",
    "        d['Low_'+ticker].fillna(d['Close_'+ticker],inplace=True)   # fill missing Low with forward filled Close\n",
    "        d['High_'+ticker].fillna(d['Close_'+ticker],inplace=True)  # fill missing High with forward filled Close\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    def add_vix(self):\n",
    "        '''\n",
    "        merge ViX OHLCV data with main instrument to be predicted\n",
    "        \n",
    "        '''\n",
    "        # merge with vix if applicable\n",
    "        if self.vix is not None:\n",
    "            _data = pn.merge(self.inst.data,self.vix.data,left_index=True,\n",
    "                                            right_index=True, how='left', \n",
    "                                            suffixes=('_'+self.inst.ticker,'_'+self.vix.ticker))\n",
    "            _data = self.fill_OHLCV_missing_data(_data,self.vix.ticker)\n",
    "            _data.dropna(inplace=True)\n",
    "            self.cleansed_data = _data\n",
    "        else:\n",
    "            self.cleansed_data = self.inst.data.add_suffix('_'+self.inst.ticker)\n",
    "    \n",
    "    \n",
    "    def number_of_bars_per_day(self,d):\n",
    "        '''\n",
    "        returns max number of records per day in a dataframe (d)\n",
    "        '''\n",
    "        return d.groupby(d.index.date).count().max().max()\n",
    "\n",
    "    def set_frequency(self,start_time=\"00:00:00\",end_time=\"23:59:59\",exclude_weekends=True):\n",
    "        \"\"\"\n",
    "        this method overrides instrument class method to handle having two OHLCV instruments merged in the \n",
    "        same DataFrame\n",
    "        \n",
    "        Given a dataframe containing OHLCV intraday price data, it returns a Dataframe with a specified \n",
    "        frequency. It is possible to remove bars outside a specific time range and exlude weekends \n",
    "        -------------------------------------------------------------------------------------\n",
    "         -------------------------------------------------------------------------------------\n",
    "         \n",
    "        Inputs:\n",
    "        -------\n",
    "        d: dataframe\n",
    "            OHLCV dataframe with 2 intruments\n",
    "        start_time: string\n",
    "            start of official trading session - %H:%M:%S\" format - 24 hour clock- (default=\"00:00:00\") \n",
    "        end_time: string\n",
    "            end of official trading session - %H:%M:%S\" format - 24 hour clock - (default=\"23:59:99\")\n",
    "        exclude_weekends: bool \n",
    "            if True, weekends are removed\n",
    "        \"\"\"\n",
    "        start_time = datetime.strptime(start_time,\"%H:%M:%S\").time()\n",
    "        end_time = datetime.strptime(end_time,\"%H:%M:%S\").time()\n",
    "        highest_dayofweek = 5 if exclude_weekends else 8\n",
    "        \n",
    "        d = self.cleansed_data\n",
    "        _ticker = self.inst.ticker \n",
    "        \n",
    "        \n",
    "        d = d[(d.index.time >= start_time) & (d.index.time < end_time) & (d.index.dayofweek < highest_dayofweek)]\n",
    "        \n",
    "        out = pn.DataFrame()\n",
    "        \n",
    "        #instrument\n",
    "        out['High_'+_ticker]     = d['High_'+_ticker].resample(self.freq).max()\n",
    "        out['Low_'+_ticker]      = d['Low_'+_ticker].resample(self.freq).min()\n",
    "        out['Open_'+_ticker]     = d['Open_'+_ticker].resample(self.freq).first()\n",
    "        out['Close_'+_ticker]    = d['Close_'+_ticker].resample(self.freq).last()\n",
    "        out['Volume_'+_ticker]   = d['Volume_'+_ticker].resample(self.freq).sum()\n",
    "        \n",
    "        #vix\n",
    "        if self.vix is not None:\n",
    "            _vix =self.vix.ticker\n",
    "            out['High_'+_vix]     = d['High_'+_vix].resample(self.freq).max()\n",
    "            out['Low_'+_vix]      = d['Low_'+_vix].resample(self.freq).min()\n",
    "            out['Open_'+_vix]     = d['Open_'+_vix].resample(self.freq).first()\n",
    "            out['Close_'+_vix]    = d['Close_'+_vix].resample(self.freq).last()\n",
    "            out['Volume_'+_vix]   = d['Volume_'+_vix].resample(self.freq).sum()\n",
    "        \n",
    "        out.dropna(inplace=True) \n",
    "        \n",
    "        self.cleansed_data = out\n",
    "    \n",
    "    def RollingOBV(self, data, ticker, timeperiod):\n",
    "        '''\n",
    "        Calculate rolling On-Balance-Volume Indicator (OBV)\n",
    "        \n",
    "        Inputs:\n",
    "            data: DataFrame\n",
    "                DataFrame containing inputs for OBV (Close and Volume)\n",
    "            ticker: str \n",
    "                ticker name\n",
    "            timeperiod: int\n",
    "                lenght of rolling window\n",
    "        '''\n",
    "        \n",
    "        df = pn.DataFrame(index=data.index)\n",
    "        df['up_bar'] = np.where(data['Close_'+ticker] > data['Close_'+ticker].shift(1),1.,-1.)\n",
    "        df['volume'] = data['Volume_'+ticker]\n",
    "        \n",
    "        return df.product(axis=1).rolling(timeperiod).sum()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def create_features(self,n=[2,5,10,25,50,100,250]):\n",
    "        \"\"\"\n",
    "        Given a dataframe with OHLCV price data, it returns a Dataframe including multiple\n",
    "        features for intraday ML forecasting\n",
    "        -------------------------------------------------------------------------------------\n",
    "        -------------------------------------------------------------------------------------\n",
    "        \n",
    "        Inputs:\n",
    "        -------\n",
    "        d: DataFrame\n",
    "            dataframe with a DateTime index containing OHLVC data for one or multiple instruments \n",
    "            labelled as \"Close_\"&ticker\n",
    "        n:  list of ints\n",
    "            lenght/s of rolling window/s\n",
    "        \"\"\"\n",
    "        \n",
    "        d = self.cleansed_data\n",
    "        tickers = [self.inst.ticker]\n",
    "        \n",
    "        out = pn.DataFrame(index=d.index)\n",
    "        \n",
    "        if self.vix is not None:\n",
    "            tickers.insert(0,self.vix.ticker)\n",
    "        \n",
    "        bars_per_day = self.number_of_bars_per_day(d)\n",
    "        \n",
    "        for t in tickers:\n",
    "            if t in 'VX':\n",
    "                out[f\"Close_{t}\"] = d['Close_'+t]\n",
    "            out[f\"d_1_{t}\"] = np.where(d['Close_'+t] - d['Close_'+t].shift(1) > 0, 1, 0)\n",
    "            out[f\"hl_1_{t}\"] =  d['High_'+t] - d['Low_'+t]\n",
    "            out[f\"hc_1_{t}\"] =  d['High_'+t] - d['Close_'+t]\n",
    "            out[f\"cl_1_{t}\"] =  d['Close_'+t] - d['Low_'+t]\n",
    "            out[f\"co_1_{t}\"] =  d['Close_'+t] - d['Open_'+t]\n",
    "            out[f\"ho_1_{t}\"] =  d['High_'+t] - d['Open_'+t]\n",
    "            out[f\"ol_1_{t}\"] =  d['Open_'+t] - d['Low_'+t]\n",
    "            out[f\"gap_1_{t}\"]= d['Open_'+t] - d['Close_'+t].shift(1)\n",
    "            out[f\"bop_{t}\"] = getattr(talib, 'BOP')(d['Open_'+t], d['High_'+t],d['Low_'+t],d['Close_'+t])\n",
    "            \n",
    "            for l in [1] + n:\n",
    "                out[f\"r_{str(l)}_{t}\"] = np.log(d['Close_'+t] / d['Close_'+t].shift(l))    # log return\n",
    "                out[f\"v_{str(l)}_{t}\"] = d['Volume_'+t].rolling(l).mean()\n",
    "                if l>1:\n",
    "                    if t == self.inst.ticker:\n",
    "                        if self.vix is not None:\n",
    "                            out[f\"std_vx_ratio{str(l)}_{t}\"] = out[f\"r_1_{t}\"].rolling(l).std()*math.sqrt(bars_per_day*252)*100./out[f\"Close_VX\"]\n",
    "                    out[f\"std_{str(l)}_{t}\"] = out[f\"r_1_{t}\"].rolling(l).std()*math.sqrt(bars_per_day*252)\n",
    "                    out[f\"d_{str(l)}_{t}\"] = out[f\"d_1_{t}\"].rolling(l).mean()\n",
    "                    out[f\"max_{str(l)}_{t}\"] = d[f\"High_{t}\"].rolling(l).max() - d['Close_'+t]\n",
    "                    out[f\"min_{str(l)}_{t}\"] = d['Close_'+t] - d[f\"Low_{t}\"].rolling(l).min()\n",
    "                    out[f\"range_{str(l)}_{t}\"] = out[f\"hl_1_{t}\"].rolling(l).mean()\n",
    "                    out[f\"hc_{str(l)}_{t}\"] = out[f\"hc_1_{t}\"].rolling(l).mean()\n",
    "                    out[f\"cl_{str(l)}_{t}\"] = out[f\"cl_1_{t}\"].rolling(l).mean()\n",
    "                    out[f\"gap_{str(l)}_{t}\"] = out[f\"gap_1_{t}\"].rolling(l).mean()\n",
    "                    out[f\"co_{str(l)}_{t}\"] =   out[f\"co_1_{t}\"].rolling(l).mean()\n",
    "                    out[f\"dsma_{str(l)}_{t}\"] = d['Close_'+t] - d[f\"Close_{t}\"].rolling(l).mean()\n",
    "                if l>2:\n",
    "                    out[f\"adx_{str(l)}_{t}\"] = getattr(talib, 'ADX')(d['High_'+t], d['Low_'+t], d['Close_'+t],timeperiod=l) # ADX \n",
    "                    out[f\"apo_{str(l)}_{t}\"] = getattr(talib, 'APO')(d['Close_'+t],fastperiod=l,slowperiod=l*2) # Absolute Price Oscillator \n",
    "                    out[f\"aronosc_{str(l)}_{t}\"] = getattr(talib, 'AROONOSC')(d['High_'+t], d['Low_'+t],timeperiod=l) # Aron Oscillator \n",
    "                    out[f\"cci_{str(l)}_{t}\"] = getattr(talib, 'CCI')(d['High_'+t], d['Low_'+t], d['Close_'+t],timeperiod=l) # CCI\n",
    "                    out[f\"mfi_{str(l)}_{t}\"] = getattr(talib, 'MFI')(d['High_'+t], d['Low_'+t], d['Close_'+t],d['Volume_'+t],timeperiod=l) #  Money Flow Index\n",
    "                    out[f\"rsi_{str(l)}_{t}\"] = getattr(talib, 'RSI')(d['Close_'+t],timeperiod=l) # Absolute Price Oscillator\n",
    "                    out[f\"obv_{str(l)}_{t}\"] = self.RollingOBV(d, t, l) # rolling OBV\n",
    "                    out[f\"natr_{str(l)}_{t}\"] = getattr(talib, 'NATR')(d['High_'+t], d['Low_'+t], d['Close_'+t],timeperiod=l) # normalised ATR\n",
    "\n",
    "        \n",
    "        #Seasonality and microseasonality    \n",
    "        out['DayOfWeek'] = d.index.day_of_week\n",
    "        if self.freq is not '1d': \n",
    "            out['Hour']  = d.index.hour\n",
    "        \n",
    "        out.dropna(inplace=True)\n",
    "        self.number_of_features = len(out.columns)\n",
    "        self.features_data = out\n",
    "\n",
    "    def create_label(self,target_up=0.25,target_down=-0.25,n=1):\n",
    "        \"\"\"\n",
    "        Given a dataframe d with OHLCV price data, it returns a timeseries with the following labels:\n",
    "            long_signal:\n",
    "                1: The Close of the next n bars is greater or equal than the next bar Open + target_up \n",
    "                0: Otherwise\n",
    "            short_signal:\n",
    "                1: The Close of the next n bars is less  than the next bar Open + target_down\n",
    "                0: Otherwise\n",
    "        -------------------------------------------------------------------------------------\n",
    "        -------------------------------------------------------------------------------------\n",
    "        \n",
    "        Inputs:\n",
    "        -------\n",
    "        target_up:  float\n",
    "            target for long signal\n",
    "        target_down: float\n",
    "            target for short signal\n",
    "        n: int\n",
    "            number of bars ahead\n",
    "        \"\"\"\n",
    "        \n",
    "        d = self.cleansed_data\n",
    "        \n",
    "        out = pn.DataFrame(index=d.index)\n",
    "        \n",
    "        t = '_' + self.inst.ticker\n",
    "        \n",
    "        future_close = d['Close'+t].shift(-n)\n",
    "        \n",
    "        out['long_label'] = np.where(future_close - d['Open'+t].shift(-1) > target_up ,1,0)\n",
    "        out['short_label'] = np.where(future_close - d['Open'+t].shift(-1) <  target_down ,1,0)\n",
    "        \n",
    "        out.dropna(inplace=True)\n",
    "        \n",
    "        self.labels_data = out\n",
    "        self.max_bars_per_day = self.number_of_bars_per_day(out)\n",
    "        self.n = n\n",
    "      \n",
    "    def standardise(self,train_,test_,clip=None):\n",
    "        \"\"\"\n",
    "        Standardise all columns of training and test DataFrames for ML predictions. \n",
    "        Test standardisation is based on mean and standard deviation of thr training data \n",
    "        -------------------------------------------------------------------------------------\n",
    "        -------------------------------------------------------------------------------------\n",
    "        \n",
    "        Inputs:\n",
    "        -------\n",
    "        train: DataFrame\n",
    "            dataframe containin the ML predictor\n",
    "        test: Dataframe  \n",
    "            dataframe with the same columns of train used to test the prediction\n",
    "        clip: float or None\n",
    "            if not None, standardised values are clipped at +- the clip value\n",
    "            \n",
    "        \"\"\" \n",
    "        \n",
    "        out_train = pn.DataFrame(index=train_.index)\n",
    "        out_test = pn.DataFrame(index=test_.index)\n",
    "        \n",
    "        for column in train_:\n",
    "            mu, std = train_[column].mean(), train_[column].std()\n",
    "            out_train[column] = (train_[column] - mu) / std\n",
    "            out_test[column] = (test_[column] - mu) / std\n",
    "        if (clip is not None):\n",
    "            out_train = out_train.clip(-clip,clip)\n",
    "            out_test = out_test.clip(-clip,clip)\n",
    "        \n",
    "        return out_train, out_test\n",
    "    \n",
    "    \n",
    "    def split_data_standardise(self, dates, clip):\n",
    "        '''\n",
    "        split features and labels into training and test and standardises features\n",
    "        \n",
    "        Inputs:\n",
    "        ----------------------\n",
    "        \n",
    "        dates: list of DateTimes [date1,date2,date3]\n",
    "                training > date1 and <=date2 , test > date2 and <= date3\n",
    "        \n",
    "        clip: float or None\n",
    "            if not None, standardised values are clipped at +- the clip value\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        features = self.features_data\n",
    "        labels   = self.labels_data\n",
    "        \n",
    "        self.features_train = features.loc[(features.index > dates[0]) & (features.index <= dates[1])]\n",
    "        self.labels_train = labels.loc[(labels.index > dates[0]) & (labels.index <= dates[1])]\n",
    "        self.features_test = features.loc[(features.index > dates[1]) & (features.index <= dates[2])]\n",
    "        self.labels_test = labels.loc[(labels.index > dates[1]) & (labels.index <= dates[2])]\n",
    "                    \n",
    "        #standardise features\n",
    "        self.features_train, self.features_test = self.standardise(self.features_train, self.features_test,\n",
    "                                                                  clip)        \n",
    "        #align features and lables dataframes\n",
    "        self.features_train, self.labels_train = self.features_train.align(self.labels_train, join='inner',axis=0)\n",
    "        self.features_test, self.labels_test = self.features_test.align(self.labels_test, join='inner',axis=0)\n",
    "        \n",
    "    def set_trading_hours(self,start_time=\"00:00:00\",end_time=\"23:59:59\"):\n",
    "        '''\n",
    "        remove bars outside before start_time and after end_time so that the ML model \n",
    "        is calibrated on the same time range of the trading strategy\n",
    "        \n",
    "        Inputs:\n",
    "        ----------------------\n",
    "        start_time: str: HH:MM:SS\n",
    "        end_time  : str: HH:MM:SS\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        start_time = datetime.strptime(start_time,\"%H:%M:%S\").time()\n",
    "        end_time = datetime.strptime(end_time,\"%H:%M:%S\").time()\n",
    "        \n",
    "        self.features_train = self.features_train[(self.features_train.index.time >= start_time) & (self.features_train.index.time < end_time)]\n",
    "        self.labels_train = self.labels_train[(self.labels_train.index.time >= start_time) & (self.labels_train.index.time < end_time)]\n",
    "        self.features_test = self.features_test[(self.features_test.index.time >= start_time) & (self.features_test.index.time < end_time)]\n",
    "        self.labels_test = self.labels_test[(self.labels_test.index.time >= start_time) & (self.labels_test.index.time < end_time)]\n",
    "        \n",
    "        \n",
    "    def features_scatter(self,cols):\n",
    "        '''\n",
    "        display multivariate scatter plot of selected features\n",
    "        \n",
    "        Inputs:\n",
    "        ----------------------\n",
    "        cols: list\n",
    "             features to be displayed\n",
    "        '''\n",
    "        \n",
    "        g = sns.PairGrid(self.features_train,vars=cols)\n",
    "        g.map_diag(sns.histplot)\n",
    "        g.map_offdiag(sns.scatterplot)\n",
    "    \n",
    "    def joint_plot(self,x1,x2,long=True):\n",
    "        '''\n",
    "        display joint plot of two selected features (x1,x2) against positive and negative label distributions\n",
    "        \n",
    "        Inputs:\n",
    "        ----------------------\n",
    "        x1: str\n",
    "             name of first feature\n",
    "        x2: str \n",
    "            name of second feature\n",
    "        long: bool\n",
    "            long label?\n",
    "        '''\n",
    "        \n",
    "        label_type = 'long_label' if long==True else 'short_label'\n",
    "        pos_df = pn.DataFrame(self.features_train.loc[self.labels_train[label_type]>=0.5], columns=self.features_train.columns)\n",
    "        neg_df = pn.DataFrame(self.features_train.loc[self.labels_train[label_type]<0.5], columns=self.features_train.columns)\n",
    "        \n",
    "        sns.jointplot(x= pos_df[x1], y =pos_df[x2],\n",
    "                          kind='hex')\n",
    "        plt.suptitle(\"Positive distribution\")\n",
    "        \n",
    "        sns.jointplot(x =neg_df[x1], y = neg_df[x2],\n",
    "                          kind='hex')\n",
    "        _ = plt.suptitle(\"Negative distribution\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7083a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    '''\n",
    "    Base class for a ML Predictor object that contains main parameters and methods shared across\n",
    "    different ML techniques\n",
    "    \n",
    "    Attributes:\n",
    "        Inputs\n",
    "        --------------------\n",
    "        optimiser: str\n",
    "            'adam' or 'sgd'\n",
    "        ephocs: int\n",
    "            numerber of ephocs\n",
    "        long: bool\n",
    "            if true, it predicts long label - otherwise the short label is used\n",
    "        early_monitor: str\n",
    "            performance metric for early stopping\n",
    "        strategy_data: strategy data class\n",
    "            cleansed data for model calibration and backtesting\n",
    "        selected_features: list containing column names to be used for model calibration.If None, all the fetures in the\n",
    "            strategy data class are used\n",
    "            \n",
    "            \n",
    "        Data:\n",
    "        ----------------------------------\n",
    "        label_bars_ahead: int \n",
    "            number of bars ahead used to contruct label\n",
    "        boruta_selector: obj\n",
    "            calibrated boruta object\n",
    "        selected_features: list \n",
    "                list containing column names to be used for a model calibration based on a more parsimonious number \n",
    "                of features. The list can be an input of the class or the output of the variable selection method. \n",
    "                If None, all generated features are used for model calibration\n",
    "    \n",
    "    Methods:\n",
    "        set_metrics:\n",
    "            defines performance metrics of ML algorithm\n",
    "        set_optimiser:\n",
    "            sets optimiser (adam or SGD)\n",
    "        cw:\n",
    "            calculates weights for label class imbalance\n",
    "        set_seeds:\n",
    "            set seeds\n",
    "        set_early_stopping:\n",
    "            set early stopping settings of calibration alghoritm\n",
    "        choose_label:\n",
    "            returns long or short label for ML fit \n",
    "        plot_metrics_history:\n",
    "            plot performance metrics across ephocs\n",
    "        plot_cm:\n",
    "            plot confusion matrices\n",
    "        plot_prc:\n",
    "            plot precision recall curve\n",
    "        plot_roc:\n",
    "            plot roc curve\n",
    "        plot_metrics:\n",
    "            plot confusion matrices, prc and roc\n",
    "        print_model\n",
    "             print model structure\n",
    "        calculate_gini\n",
    "            calculate gini for test and training\n",
    "        boruita_fit\n",
    "            fit boruta method for feature selection - store result in prperty boruta_selector\n",
    "        features_selection\n",
    "            generate a list of slected features based on Boruta approach\n",
    "            \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,optimiser,epochs,long,early_monitor,strategy_data,selected_features):\n",
    "        self.set_optimiser(optimiser)\n",
    "        self.set_metrics()\n",
    "        self.set_early_stopping(early_monitor)\n",
    "        self.set_seeds()\n",
    "        self.model = None\n",
    "        self.epochs = epochs\n",
    "        self.long = long\n",
    "        self.long_str = 'Long Label' if long else 'Short Label'\n",
    "        self.label_bars_ahead = strategy_data.n\n",
    "        self.selected_features = strategy_data.features_train.columns if selected_features is None else selected_features\n",
    "\n",
    "        \n",
    "    def set_metrics(self):\n",
    "        '''\n",
    "        Defines Performance metrics of ML calibration\n",
    "        '''\n",
    "        self.METRICS = [\n",
    "          keras.metrics.TruePositives(name='tp'),\n",
    "          keras.metrics.FalsePositives(name='fp'),\n",
    "          keras.metrics.TrueNegatives(name='tn'),\n",
    "          keras.metrics.FalseNegatives(name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "        ]\n",
    "    \n",
    "    def set_optimiser(self, name, lr=0.001, momentum=0., nesterov=False):\n",
    "        '''\n",
    "        set optimiser\n",
    "        '''\n",
    "        if name == 'adam': \n",
    "            self.optimiser = Adam(lr = lr)\n",
    "        elif name == 'sgd':\n",
    "            self.optimiser = SGD(lr=lr, momentum=momentum, nesterov=nesterov)\n",
    "        else:\n",
    "            self.optimiser = name\n",
    "    \n",
    "    def cw(self,ts):\n",
    "        '''\n",
    "        Calculate weights for label's class imbalance\n",
    "        '''\n",
    "        c0, c1 = np.bincount(ts)\n",
    "        w0 = (1 / c0) * (len(ts)) / 2\n",
    "        w1 = (1 / c1) * (len(ts)) / 2\n",
    "        return {0: w0, 1: w1}\n",
    "        \n",
    "    def set_seeds(self,seed=100):\n",
    "        '''\n",
    "        set random seeds to generate the same results\n",
    "        '''\n",
    "        random.seed(seed)  \n",
    "        np.random.seed(seed)  \n",
    "        tf.random.set_seed(seed)\n",
    "    \n",
    "    def set_early_stopping(self,monitor):\n",
    "        self.early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=monitor, \n",
    "        verbose=1,\n",
    "        patience=40,\n",
    "        mode='min',\n",
    "        restore_best_weights =False)\n",
    "        \n",
    "    def choose_label(self,strategy_data, long=True):\n",
    "        '''\n",
    "        return long (or short) training and test labels\n",
    "        \n",
    "        inputs:\n",
    "            strategy_data: strategy_data class\n",
    "            long:bool\n",
    "                if true , the long label is returned - false for short\n",
    "        '''\n",
    "        if long == True:\n",
    "            label = strategy_data.labels_train['long_label']\n",
    "            label_test = strategy_data.labels_test['long_label']\n",
    "        else:\n",
    "            label = strategy_data.labels_train['short_label']\n",
    "            label_test = strategy_data.labels_test['short_label']\n",
    "        \n",
    "        return label, label_test\n",
    "        \n",
    "    def plot_metrics_history(self,history):\n",
    "        '''\n",
    "        plot metrics by ephocs\n",
    "        '''\n",
    "        out = pn.DataFrame(history)\n",
    "        metrics = ['loss', 'accuracy', 'precision', 'recall']\n",
    "        plt.figure(figsize=(12,8))\n",
    "        for n, metric in enumerate(metrics):\n",
    "            name = metric.replace(\"_\",\" \").capitalize()\n",
    "            plt.subplot(2,2,n+1)\n",
    "            plt.title(str(metric)+'_'+ self.long_str)\n",
    "            plt.plot(out.index, out[metric], color='blue', label='Train')\n",
    "            plt.plot(out.index, out['val_'+metric],\n",
    "                     color='green', linestyle=\"--\", label='Val')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(name)\n",
    "            if metric == 'loss':\n",
    "                plt.ylim([0, plt.ylim()[1]])\n",
    "            elif metric == 'auc':\n",
    "                plt.ylim([0.5,1])\n",
    "            else:\n",
    "                plt.ylim([0,1])\n",
    "            plt.legend()  \n",
    "    \n",
    "    def plot_cm(self,labels, predictions,name='Training', p=0.5):\n",
    "        '''\n",
    "        plot confusion matrix\n",
    "        '''\n",
    "        cm = confusion_matrix(labels, predictions > p)\n",
    "        cm = cm / np.sum(cm)\n",
    "        plt.figure(figsize=(5,5))\n",
    "        sns.heatmap(cm, annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "        plt.title(self.long_str + ' ' + name + ' Confusion matrix @{:.2f}'.format(p))\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "    \n",
    "    def plot_prc(self,name, labels, predictions, **kwargs):\n",
    "        '''\n",
    "        plot precision recall curve\n",
    "        '''\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)\n",
    "        plt.plot(precision, recall, label=name, linewidth=2, **kwargs)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.grid(True)\n",
    "        ax = plt.gca()\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "    def plot_roc(self,name, labels, predictions, **kwargs):\n",
    "        '''\n",
    "        plot roc curve\n",
    "        '''\n",
    "        fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "        print('GINI '+name +': ' + str(sklearn.metrics.auc(fp, tp)*2.-1.))\n",
    "        plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "        plt.xlabel('False positives [%]')\n",
    "        plt.ylabel('True positives [%]')\n",
    "        plt.grid(True)\n",
    "        ax = plt.gca()\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "    def plot_metrics(self,train_labels,train_predictions,test_labels,test_predictions,p_=0.5):\n",
    "        '''\n",
    "        plot confusion matrices, precision recall curve, roc curve \n",
    "        '''\n",
    "        self.plot_cm(train_labels,train_predictions,p=p_)\n",
    "        self.plot_cm(test_labels,test_predictions,name='Test',p=p_)\n",
    "    \n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.subplot(1,2,1)\n",
    "        self.plot_prc(self.long_str + \" Train\", train_labels, train_predictions, color='blue')\n",
    "        self.plot_prc(self.long_str + \" Test\", test_labels, test_predictions, color='green', linestyle='--')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title(self.long_str + ' Precision-Recall Curve')\n",
    "        plt.subplot(1,2,2)\n",
    "        self.plot_roc(self.long_str + \" Train\", train_labels, train_predictions, color='blue')\n",
    "        self.plot_roc(self.long_str + \" Test\", test_labels, test_predictions, color='green', linestyle='--')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title(self.long_str + ' ROC Curve')\n",
    " \n",
    "    def calculate_confusion_matrix(self,labels,predictions, p=0.5):\n",
    "        \n",
    "        if confusion_matrix(labels, predictions > p).ravel().size == 4:\n",
    "            tn, fp, fn, tp = confusion_matrix(labels, predictions > p).ravel()\n",
    "            accuracy = (tp + tn) / (tn + fp + fn + tp)\n",
    "            \n",
    "            if (tp + fp) > 0:\n",
    "                precision = tp / (tp + fp)\n",
    "            else:\n",
    "                precision = None\n",
    "            \n",
    "            if (tp + fn) > 0:\n",
    "                recall = tp / (tp + fn)\n",
    "            else:\n",
    "                recall = None\n",
    "        else:\n",
    "                accuracy = None\n",
    "                precision = None\n",
    "                recall = None\n",
    "        \n",
    "        return accuracy,precision,recall\n",
    "    \n",
    "    def calculate_gini(self,train_labels,train_predictions,test_labels,test_predictions):\n",
    "        '''\n",
    "        calculate gini of train and test\n",
    "        '''\n",
    "        fp, tp, _ = sklearn.metrics.roc_curve(train_labels, train_predictions)\n",
    "        self.train_gini = sklearn.metrics.auc(fp, tp)*2.-1.   \n",
    "        \n",
    "        self.train_accuracy, self.train_precision, self.train_recall = self.calculate_confusion_matrix(train_labels,\n",
    "                                                                                                       train_predictions)\n",
    "        fp, tp, _ = sklearn.metrics.roc_curve(test_labels, test_predictions)\n",
    "        self.test_gini = sklearn.metrics.auc(fp, tp)*2.-1.  \n",
    "        \n",
    "        self.test_accuracy, self.test_precision, self.test_recall = self.calculate_confusion_matrix(test_labels,\n",
    "                                                                                                       test_predictions)\n",
    "    \n",
    "    def print_model(self):\n",
    "        '''\n",
    "        print model summary\n",
    "        '''\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    def boruta_fit(self,strategy_data):\n",
    "        '''\n",
    "        Apply Boruta approach for variable selection  \n",
    "        \n",
    "        inputs:\n",
    "            strategy_data: strategy_data class\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        y,_ = self.choose_label(strategy_data, long = self.long)\n",
    "        rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "        boruta_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42)\n",
    "        boruta_selector.fit(strategy_data.features_train.values,y)\n",
    "        self.boruta_selector = boruta_selector\n",
    "    \n",
    "    def boruta_features_selection(self,strategy_data,ranking_thr = None):\n",
    "        '''\n",
    "        save selected features based on boruta method\n",
    "        \n",
    "        inputs:\n",
    "            strategy_data: strategy_data class        \n",
    "            ranking_thr: int \n",
    "                select features that rank higher than ranking_thr. If None, use Boruta selected features \n",
    "        '''\n",
    "            \n",
    "        if ranking_thr == None:\n",
    "            self.selected_features = strategy_data.features_train.columns[self.boruta_selector.support_]  \n",
    "        else:\n",
    "            self.selected_features = strategy_data.features_train.columns[self.boruta_selector.ranking_<=ranking_thr]\n",
    "        \n",
    "    def drop_correlated_features(self,strategy_data,corr_threshold = 0.7):\n",
    "        '''\n",
    "        remove from selected features list the features with correlation higher than threshold\n",
    "        \n",
    "        inputs:\n",
    "            strategy_data: strategy_data class        \n",
    "            corr_threshold: float \n",
    "                correlation threshold \n",
    "        '''\n",
    "        \n",
    "        c = strategy_data.features_train[self.selected_features].corr()\n",
    "\n",
    "        # Identify highly correlated features\n",
    "        correlated_features = set()\n",
    "        for i in range(len(c.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(c.iloc[i, j]) > corr_threshold:\n",
    "                    colname = c.columns[i]\n",
    "                    correlated_features.add(colname)\n",
    "        # remove highly correlated features\n",
    "        self.selected_features = self.selected_features.drop(correlated_features)\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f194f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(Predictor):\n",
    "    '''\n",
    "    Dense Neural Network class\n",
    "    \n",
    "    Attributes:\n",
    "        Inputs\n",
    "        --------------------\n",
    "        strategy_data: strategy data class\n",
    "            class containing the strategy data\n",
    "        optimiser: str\n",
    "            'adam' or 'sgd'\n",
    "        hl: int\n",
    "            number of hidden layers\n",
    "        hu: int\n",
    "            number of hidden units\n",
    "        dropout: bool\n",
    "            if trueapply random dropout of nodes\n",
    "        rate: float\n",
    "            dropout rate if dropout is true\n",
    "        regularize: bool\n",
    "            regularize nodes\n",
    "        reg_l1=float\n",
    "            regularisation l2 parameter\n",
    "        ephocs: int\n",
    "            number of ephocs\n",
    "        long: bool\n",
    "            if true - long label is used\n",
    "        early_monitor: str\n",
    "            performance metric for early stopping   \n",
    "            \n",
    "        Data:\n",
    "        ----------------------------------------\n",
    "        model: sequential object\n",
    "            tensor flow mdel\n",
    "        results: \n",
    "            model results\n",
    "        train_predict:\n",
    "            training prediction probability\n",
    "        test_predict:\n",
    "            test prediction probability\n",
    "        train_label:\n",
    "            training labels\n",
    "        test_label:\n",
    "            test_labels\n",
    "        train_gini:\n",
    "            gini of training\n",
    "        test_gini:\n",
    "            gini of test\n",
    "        \n",
    "        Methods:\n",
    "        -----------------------------------------\n",
    "        create model\n",
    "        fit\n",
    "        predict\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,strategy_data,optimiser='adam',hl=1,hu=128,dropout=False,rate=0.3,regularize=False,\n",
    "                reg_l1=0.0005,epochs=100,long=True,early_monitor='val_loss', selected_features = None):\n",
    "        super().__init__(optimiser = optimiser, epochs = epochs, long = long, \n",
    "                         early_monitor = early_monitor, strategy_data = strategy_data, \n",
    "                         selected_features = selected_features)\n",
    "        self.strategy_data = strategy_data\n",
    "        self.hl = hl\n",
    "        self.hu = hu\n",
    "        self.dropout = dropout\n",
    "        self.rate = rate\n",
    "        self.regularize = regularize\n",
    "        self.reg_l1=reg_l1\n",
    "        self.train_label,self.test_label = self.choose_label(strategy_data,self.long)\n",
    "        \n",
    "    def create_model(self,hl, hu, dropout, rate, regularize, reg, optimiser, \n",
    "                     input_dim, metrics):\n",
    "        '''\n",
    "        create NN model\n",
    "        '''\n",
    "        \n",
    "        if not regularize:\n",
    "            reg = None\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hu, input_dim=input_dim,\n",
    "                        activity_regularizer=reg,  \n",
    "                        activation='relu'))\n",
    "        if dropout:\n",
    "            model.add(Dropout(rate, seed=100))\n",
    "        for _ in range(hl):\n",
    "            model.add(Dense(hu, activation='relu',\n",
    "                            activity_regularizer=reg))  \n",
    "            if dropout:\n",
    "                model.add(Dropout(rate, seed=100))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimiser,\n",
    "                      metrics= metrics)\n",
    "        self.model = model\n",
    "    \n",
    "    \n",
    "    def fit(self,strategy_data):\n",
    "        '''\n",
    "        fit model\n",
    "        '''\n",
    "        \n",
    "        self.create_model(self.hl,self.hu,self.dropout,self.rate,self.regularize,\n",
    "                          l1(self.reg_l1),self.optimiser,len(self.selected_features),self.METRICS)\n",
    "        \n",
    "        self.results =  self.model.fit(strategy_data.features_train[self.selected_features],\n",
    "                                        self.train_label,epochs=self.epochs, \n",
    "                                        verbose=False,validation_split=0.3, shuffle=False,\n",
    "                                        class_weight=self.cw(self.train_label),\n",
    "                                        callbacks=[self.early_stopping])\n",
    "            \n",
    "            \n",
    "    def predict(self,strategy_data,show_charts=False):\n",
    "        '''\n",
    "        predict training and out of sample\n",
    "        '''\n",
    "        self.train_predict = pn.Series(self.model.predict(strategy_data.features_train[self.selected_features]).flatten(), \n",
    "                                       index = self.train_label.index)\n",
    "        \n",
    "        self.test_predict = pn.Series(self.model.predict(strategy_data.features_test[self.selected_features]).flatten(), \n",
    "                                      index = self.test_label.index)\n",
    "        \n",
    "        self.calculate_gini(self.train_label,self.train_predict,self.test_label,self.test_predict)\n",
    "        \n",
    "        if show_charts:\n",
    "            self.plot_metrics_history(self.results.history)\n",
    "            self.plot_metrics(self.train_label,self.train_predict,self.test_label,self.test_predict)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4288768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRNN(Predictor):\n",
    "    '''\n",
    "    Dense Recurrent Neural Network class\n",
    "    \n",
    "    Attributes:\n",
    "        Inputs\n",
    "        --------------------\n",
    "        strategy_data: strategy data class\n",
    "            class containing the strategy data\n",
    "        lags: int\n",
    "            number of lags\n",
    "        optimiser: str\n",
    "            'adam' or 'sgd'\n",
    "        hl: int\n",
    "            number of hidden layers\n",
    "        hu: int\n",
    "            number of hidden units\n",
    "        layer: str\n",
    "            \"SimpleRNN\" or \"LTSM\"\n",
    "        dropout: bool\n",
    "            if trueapply random dropout of nodes\n",
    "        rate: float\n",
    "            dropout rate if dropout is true\n",
    "        ephocs: int\n",
    "            number of ephocs\n",
    "        long: bool\n",
    "            if true - long label is used\n",
    "        early_monitor: str\n",
    "            performance metric for early stopping   \n",
    "        \n",
    "        Data:\n",
    "        ----------------------------------------\n",
    "        model: sequential object\n",
    "            tensor flow mdel\n",
    "        results: \n",
    "            model results\n",
    "        train_predict:\n",
    "            training prediction probabilities\n",
    "        test_predict:\n",
    "            test prediction probabilities\n",
    "        train_label:\n",
    "            training labels\n",
    "        test_label:\n",
    "            test_labels\n",
    "            \n",
    "        Methods:\n",
    "        -----------------------------------------\n",
    "        create model\n",
    "        fit\n",
    "        predict\n",
    "\n",
    "    '''\n",
    "    def __init__(self,strategy_data,lags=5,optimiser='rmsprop',hl=1,hu=100,layer='SimpleRNN',\n",
    "                dropout=False,rate=0.3,epochs=50,long=True, early_monitor='loss',selected_features = None):\n",
    "        super().__init__(optimiser = optimiser, epochs = epochs, long = long, \n",
    "                         early_monitor = early_monitor, strategy_data = strategy_data,\n",
    "                         selected_features = selected_features)\n",
    "        self.hl = hl\n",
    "        self.hu = hu\n",
    "        self.layer = layer\n",
    "        self.dropout= dropout\n",
    "        self.rate = rate\n",
    "        self.optimiser = optimiser\n",
    "        self.lags = lags\n",
    "        self.train_label,self.test_label = self.choose_label(strategy_data,self.long)\n",
    "    \n",
    "    \n",
    "    def plot_metrics_history(self,history):\n",
    "        '''\n",
    "        plot metrics by ephocs\n",
    "        '''\n",
    "        out = pn.DataFrame(history)\n",
    "        metrics = ['loss', 'accuracy', 'precision', 'recall']\n",
    "        plt.figure(figsize=(12,8))\n",
    "        for n, metric in enumerate(metrics):\n",
    "            name = metric.replace(\"_\",\" \").capitalize()\n",
    "            plt.subplot(2,2,n+1)\n",
    "            plt.plot(out.index, out[metric], color='blue', label='Train')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(name)\n",
    "            if metric == 'loss':\n",
    "                plt.ylim([0, plt.ylim()[1]])\n",
    "            elif metric == 'auc':\n",
    "                plt.ylim([0.5,1])\n",
    "            else:\n",
    "                plt.ylim([0,1])\n",
    "            plt.legend() \n",
    "    \n",
    "    \n",
    "    def create_model(self,hl, hu, layer,optimizer, features,dropout, rate,lags_,metrics):\n",
    "        '''\n",
    "        create DRNN model\n",
    "        '''\n",
    "        if hl <= 2: \n",
    "                hl = 2  \n",
    "        if layer == 'SimpleRNN':\n",
    "            layer = SimpleRNN\n",
    "        else:\n",
    "            layer = LSTM\n",
    "        model = Sequential()\n",
    "        model.add(layer(hu, input_shape=(lags_, features),\n",
    "                                   return_sequences=True))  \n",
    "        if dropout:\n",
    "            model.add(Dropout(rate, seed=100))  \n",
    "        for _ in range(2, hl):\n",
    "            model.add(layer(hu, return_sequences=True))\n",
    "            if dropout:\n",
    "                model.add(Dropout(rate, seed=100))  \n",
    "        model.add(layer(hu))  \n",
    "        model.add(Dense(1, activation='sigmoid'))  \n",
    "        model.compile(optimizer=optimizer,\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=metrics)\n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self,strategy_data,batch_size = 5,steps_per_epoch=10):\n",
    "        \n",
    "        self.create_model(self.hl,self.hu,self.layer,self.optimiser,len(self.selected_features),\n",
    "                          self.dropout, self.rate,self.lags,self.METRICS)\n",
    "        \n",
    "        self.g = TimeseriesGenerator(strategy_data.features_train[self.selected_features], \n",
    "                                self.train_label, length=self.lags, batch_size=batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.results = self.model.fit(self.g, epochs=self.epochs, steps_per_epoch=steps_per_epoch,\n",
    "                                      verbose=False, class_weight=self.cw(self.train_label),\n",
    "                                      callbacks=[self.early_stopping])\n",
    "        \n",
    "\n",
    "    \n",
    "    def predict(self,strategy_data,batch_size = 5,show_charts=False):\n",
    "        '''\n",
    "        predict training and outof sample\n",
    "        '''\n",
    "        \n",
    "        self.train_predict = pn.Series(self.model.predict(self.g, batch_size=None).flatten(),\n",
    "                                        index = self.train_label.index[self.lags:])\n",
    "        \n",
    "        g_test = TimeseriesGenerator(strategy_data.features_test[self.selected_features], \n",
    "                                         self.test_label,length=self.lags, batch_size=batch_size)\n",
    "        \n",
    "        self.test_predict = pn.Series(self.model.predict(g_test, batch_size=None).flatten(),\n",
    "                                        index = self.test_label.index[self.lags:])\n",
    "        \n",
    "        self.train_label = self.train_label[self.lags:]\n",
    "        self.test_label = self.test_label[self.lags:]\n",
    "        \n",
    "        self.calculate_gini(self.train_label,self.train_predict,self.test_label,self.test_predict)\n",
    "        \n",
    "        if show_charts:\n",
    "            self.plot_metrics_history(self.results.history)\n",
    "            self.plot_metrics(self.train_label,self.train_predict,self.test_label,self.test_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1266f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy(object):\n",
    "    '''\n",
    "    Strategy object for vectorized backtesting\n",
    "    \n",
    "    Attributes:\n",
    "        Inputs\n",
    "        --------------------\n",
    "        strategy_data: strategy_data class \n",
    "            cleansed data for ML prediction and backtesting\n",
    "        predictor_long: predictor class\n",
    "            calibrated long ML\n",
    "        predictor_short\n",
    "            calibrated short ML object\n",
    "        p_thr: float\n",
    "            probability threshould for long and short signal\n",
    "        max_w:float\n",
    "            maximum long and short weight (ex. if equal to 1 then -1<=w<=1)\n",
    "        point_value: float\n",
    "            point value of instrument to be backtested\n",
    "        commissions: float\n",
    "            commissions per trade\n",
    "        slippage: float\n",
    "            slippage per trade\n",
    "        \n",
    "        \n",
    "        Data:\n",
    "        ----------------------------------------\n",
    "        w: DataFrame\n",
    "            strategy weights ('long','short','long_short','benchmark_long','benchmark_short')\n",
    "        gross_perf_metrics: Dataframe\n",
    "        gross performance metrics: Dataframe\n",
    "        gross_cumulative_equity: Dataframe\n",
    "        gross_log_returns: Dataframe\n",
    "        net_log_return: Dataframe\n",
    "        net_cumulative_equity: Dataframe\n",
    "        net_perf_metrics: Dataframe\n",
    "        \n",
    "            \n",
    "        Methods:\n",
    "        -----------------------------------------\n",
    "        compute_weights:\n",
    "            calculates strategy weights\n",
    "        calculate_perf_metrics\n",
    "        run_backtest\n",
    "        \n",
    "        \n",
    "\n",
    "    '''\n",
    "    def __init__(self, strategy_data, predictor_long, predictor_short,p_thr=0.5, max_w=1.,\n",
    "                point_value=50,commission=2.,slippage=0.25,show_chart=False):\n",
    "        self.max_bars_per_day = strategy_data.max_bars_per_day\n",
    "        self.long_bars_ahead = predictor_long.label_bars_ahead \n",
    "        self.short_bars_ahead = predictor_short.label_bars_ahead\n",
    "        self.max_w = max_w\n",
    "        \n",
    "        \n",
    "        self.compute_weights(predictor_long.test_predict, predictor_short.test_predict,\n",
    "                            predictor_long.label_bars_ahead, predictor_short.label_bars_ahead)\n",
    "        \n",
    "        \n",
    "        self.ticker =  strategy_data.inst.ticker \n",
    "        self.run_backtest(strategy_data.cleansed_data['Close_'+self.ticker],\n",
    "                          self.w,point_value=point_value,\n",
    "                          commission=commission,slippage=slippage,show_chart=show_chart)\n",
    "        \n",
    "    \n",
    "    def compute_weights(self,pred_long, pred_short,bars_long,bars_short,p_thr=0.5):\n",
    "        '''\n",
    "        Calculates strategy weights (long only, short only and long+short) based probabilities\n",
    "        of long (and short) label within the next \"bars_long\" (and \"bars_short\") bars\n",
    "        \n",
    "        is \"pred_long probability\" at bar (i) is higher than \"p_thr\" 1. is added to the weights \n",
    "        of the next n (\"bars_long\"). weights are then capped and flored to range between -1 and 1\n",
    "        \n",
    "        \n",
    "        Inputs:\n",
    "        --------------------------------------------------\n",
    "            pred_long: Series\n",
    "                long predicted probabilities\n",
    "            pred_short: Series\n",
    "                short predicted probabilities\n",
    "            bars_long: int\n",
    "                bars ahead for long prediction \n",
    "            bars_short: int\n",
    "                bars ahead for short prediction \n",
    "            p_thr: float\n",
    "                probability threshold for long signal\n",
    "        \n",
    "        '''\n",
    "\n",
    "        w = pn.DataFrame(index=pred_long.index.union(pred_short.index))\n",
    "\n",
    "        w['long'] = 0.    # long only strategy\n",
    "        w['short'] = 0.   # short only strategy\n",
    "        for i in range(1,bars_long+1):\n",
    "            w['long'] = w['long'] + np.where((pred_long.shift(i)>p_thr),1.,0.)\n",
    "        for i in range(1,bars_short+1):\n",
    "            w['short'] = w['short'] + np.where((pred_short.shift(i)>p_thr),-1.,0.)\n",
    "        w['long_short'] = w['long'] + w['short']  # long short strategy\n",
    "\n",
    "        w = w.clip(-1.*self.max_w,self.max_w)\n",
    "        \n",
    "        #passive strategies to be used as benchmarks\n",
    "        w['baseline_long'] = 1.\n",
    "        \n",
    "        self.w = w\n",
    "    \n",
    "    def calculate_perf_metrics(self,p,num_bars_per_day):\n",
    "        '''\n",
    "        It returns performance metrics (average return, average standard deviation, Informatio Ratio) \n",
    "        from an array of strategy returns\n",
    "        \n",
    "        Inputs:\n",
    "        -------------------------\n",
    "        p: DataFrame\n",
    "            dataframe with multiple strategies' returns\n",
    "        num_bars_per_day: int\n",
    "            number of bars per day. It is used to annualise the performance metrics \n",
    "        \n",
    "        '''\n",
    "        perf_stats = pn.DataFrame()\n",
    "        perf_stats['mean_return']=p.mean()*num_bars_per_day*252\n",
    "        perf_stats['std_return']=p.std()*math.sqrt(num_bars_per_day*252)\n",
    "        perf_stats['IR']= perf_stats['mean_return'] / perf_stats['std_return']\n",
    "        return perf_stats\n",
    "    \n",
    "    def run_backtest(self,c,w,point_value=50,commission=2.,slippage=0.25,show_chart=False):\n",
    "        '''\n",
    "        backtest trading strategies given weights, capital and transactions costs \n",
    "        \n",
    "        Inputs:\n",
    "        ----------------------------\n",
    "            c: Series\n",
    "                Close time series of the instrument that is traded \n",
    "            w: Dataframw\n",
    "                weights a verious strategies\n",
    "            initial_capital: float\n",
    "            point_value: float\n",
    "                value of 1 unit of the traded instrument\n",
    "            commission: float\n",
    "            slippage: float\n",
    "\n",
    "        '''\n",
    "\n",
    "        gross_perf = pn.DataFrame(index = c.index)\n",
    "        \n",
    "        log_ret = np.log(c / c.shift(1)) \n",
    "        \n",
    "        gross_perf['long'] = w['long'] * log_ret\n",
    "        gross_perf['short'] = w['short'] * log_ret\n",
    "        gross_perf['long_short'] = w['long_short'] * log_ret\n",
    "        gross_perf['baseline_long'] = log_ret\n",
    "        gross_perf.dropna(inplace=True)\n",
    "        \n",
    "        nn = max(self.long_bars_ahead ,self.short_bars_ahead )\n",
    "        gross_perf = gross_perf.iloc[nn:,:]\n",
    "\n",
    "        self.gross_perf_metrics = self.calculate_perf_metrics(gross_perf,self.max_bars_per_day).round(2)\n",
    "        self.gross_log_returns = gross_perf \n",
    "        self.gross_cumulative_equity = gross_perf.cumsum().apply(np.exp)\n",
    "        \n",
    "        if show_chart == True:\n",
    "            self.gross_cumulative_equity.plot(figsize=(16, 6),\n",
    "                                                   title='gross strategy returns');\n",
    "            print('*****Gross Performance*****')\n",
    "            print('')\n",
    "            print(self.gross_perf_metrics)         \n",
    "        \n",
    "        perf = pn.DataFrame(index = c.index)\n",
    "        \n",
    "        avg_p = c[gross_perf.index].mean()\n",
    "        ptr =  (commission/point_value + slippage)/avg_p #proportional transaction cost \n",
    "        self.cost = (w != w.shift(1))* 1. * ptr\n",
    "        \n",
    "        perf['long'] = w['long'] * log_ret -self.cost['long']\n",
    "        perf['short'] = w['short'] * log_ret -self.cost['short']\n",
    "        perf['long_short'] = w['long_short'] * log_ret - self.cost['long_short']\n",
    "        perf['baseline_long'] = log_ret\n",
    "        perf = perf.iloc[nn:,:]\n",
    "        perf.dropna(inplace=True)\n",
    "        \n",
    "        self.Net_log_returns = perf \n",
    "        self.net_cumulative_equity =  perf.cumsum().apply(np.exp)\n",
    "        self.net_perf_metrics = self.calculate_perf_metrics(perf,self.max_bars_per_day).round(2)\n",
    "        \n",
    "        if show_chart == True:\n",
    "            self.net_cumulative_equity.plot(figsize=(16, 6),title='net strategy cumulative returns');\n",
    "            print('*****Net Performance*****')\n",
    "            print('')\n",
    "            print(self.net_perf_metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8abd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_validation(object):\n",
    "    '''\n",
    "    this class is used to run the full process from data preparation, ML calibration, prediction and backtetsing. \n",
    "    It can be used to run a single or multiple combinations of inputs. Input and output performance \n",
    "    metrics are stored in the \"input_output_df\" dataframe. Performance charts are also generated.\n",
    "    \n",
    "    Attributes:\n",
    "        Data:\n",
    "        -----------------------------------------------\n",
    "        combo_id: int\n",
    "            id of combination of inputs\n",
    "        input_output_df: DataFrame\n",
    "            each row is a combination of inputs to be tested. the columns contain \n",
    "            input and output metrics\n",
    "            \n",
    "    Methods:\n",
    "        initialise_input_df\n",
    "        add_inputs_combo\n",
    "            add combination of inputs to inputs dataframe\n",
    "        run\n",
    "\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.initialise_input_df()\n",
    "        \n",
    "    def initialise_input_df(self):\n",
    "        '''\n",
    "        create the inputs dataframe\n",
    "        \n",
    "        list of inputs:\n",
    "        StrategyData\n",
    "            frequency: str \n",
    "                frequency of data\n",
    "            vix: bool\n",
    "                include vix\n",
    "            target_label: float\n",
    "                minimum up or down change in price (based on number of standard deviations) for label to be true\n",
    "            n label: int\n",
    "                bars ahead for label\n",
    "            clip: float\n",
    "                cap and floor of standardised variables\n",
    "            date1: datetime\n",
    "                start of training sample \n",
    "            date2: datetime\n",
    "                end of training and start of test\n",
    "            date3: datetime\n",
    "                end of test\n",
    "            start_trading_time: time\n",
    "            end_trading_date: time\n",
    "            run_boruta: bool\n",
    "                reduce number of features based on boruta\n",
    "            drop_correlated_features: bool\n",
    "                drop correlated features\n",
    "            ephocs: int\n",
    "                number of ephocs\n",
    "            long: bool \n",
    "                if true, long signal\n",
    "            ml_type: str\n",
    "                'DNN' or 'RDNN'\n",
    "            optimiser: str\n",
    "                optimiser name\n",
    "            lr: float\n",
    "                lr parameter for optimiser\n",
    "            momentum: foat\n",
    "                momentum for SGD optimiser\n",
    "            nesterow: bool \n",
    "                parameter for SGD optimiser\n",
    "            hl: int\n",
    "                hiddent layers\n",
    "            hu: int\n",
    "                hidden units\n",
    "            dropout: bool\n",
    "                dropout true false\n",
    "            rate: float\n",
    "                dropout rate\n",
    "            regularize: bool\n",
    "                regularise units\n",
    "            reg_l1:\n",
    "                l1 regularisation paramater\n",
    "            lags: int\n",
    "                number of lags for RDNN\n",
    "            layer:str\n",
    "                RDNN layer type: \"SimpleRNN\" or \"LTSM\"\n",
    "\n",
    "        '''        \n",
    "        param_names = ['frequency','vix','target_label','n_label','clip','date1','date2','date3','start_trading_time',\n",
    "                       'end_trading_time','run_boruta','drop_correlated_features','epochs','long','ml_type',\n",
    "                       'optimiser','lr','momentum','nesterov','hl','hu','dropout','rate','regularize','reg_l1',\n",
    "                       'lags','layer']\n",
    "        self.input_output_df = pn.DataFrame(columns=param_names)\n",
    "        self.combo_id=0\n",
    "        \n",
    "    \n",
    "    def to_csv(self,name):\n",
    "        self.input_output_df.to_csv(name +'.csv')\n",
    "    \n",
    "    def add_input_combo(self,frequency='30min',vix=True,target_label=1.,n_label=1,clip=5.,\n",
    "                             date1 = '2015-12-31 23:59:00',date2 = '2017-12-31 23:59:00' ,\n",
    "                             date3 = '2018-12-31 23:59:00',start_trading_time = \"00:08:00\",\n",
    "                             end_trading_time = \"17:59:59\", run_boruta = False,\n",
    "                             drop_correlated_features = False, epochs = 50,long = True, ml_type = 'DNN',\n",
    "                             optimiser = 'adam',lr = 0.001, momentum = 0.8 ,nesterov=True,hl = 1,hu=16,\n",
    "                             dropout = True, rate = 0.3,regularize = True, reg_l1 = 0.0005 ,\n",
    "                             lags = 5,layer = 'SimpleRNN'):\n",
    "        '''\n",
    "        add new combination of parameters to \"input_output_df\" dataframe. the run method will calibrate and test the \n",
    "        provided combination\n",
    "        '''\n",
    "        row = [frequency,vix,target_label,n_label,clip,date1,date2,date3,start_trading_time, end_trading_time,\n",
    "                       run_boruta, drop_correlated_features, epochs,long,ml_type,\n",
    "                       optimiser,lr,momentum,nesterov,hl,hu,dropout,rate,regularize,reg_l1,lags,layer]          \n",
    "        self.input_output_df.loc[self.combo_id] = row\n",
    "        self.combo_id = self.combo_id + 1 \n",
    "    \n",
    "    def add_multiple_input_combos(self,frequency=['30min'],vix=[True],target_label=[1.],\n",
    "                             n_label=[1],clip=[5.],\n",
    "                             date1 = ['2015-12-31 23:59:00'],date2 = ['2017-12-31 23:59:00'] ,\n",
    "                             date3 = ['2018-12-31 23:59:00'],start_trading_time = [\"00:08:00\"],\n",
    "                             end_trading_time = [\"17:59:59\"],run_boruta = [False],\n",
    "                             drop_correlated_features = [False], epochs = [50],long = [True], ml_type = ['DNN'],\n",
    "                             optimiser = ['adam'],lr = [0.001], momentum = [0.8] ,nesterov=[True],hl = [1],\n",
    "                             hu=[16], dropout = [True], rate = [0.3],regularize = [True], reg_l1 = [0.0005] ,\n",
    "                             lags = [5],layer = ['SimpleRNN']):\n",
    "        '''\n",
    "        add multiple combinations of inputs to the input_output_df dataframe. the run)multiple method will iterate across\n",
    "        the rows of the inputs dataframe to calibrate and test all of them.\n",
    "        \n",
    "        multiple values can be added to each field as list elements. a cartesian produc across all lists of inputs is\n",
    "        done to generate all the combinations\n",
    "\n",
    "        '''\n",
    "        \n",
    "        param_names = [frequency,vix,target_label,n_label,clip,date1,date2,date3,start_trading_time, \n",
    "                       end_trading_time ,run_boruta, drop_correlated_features, \n",
    "                       epochs,long,ml_type,optimiser,lr,momentum,nesterov,hl,hu,dropout,rate,\n",
    "                       regularize,reg_l1,lags,layer]\n",
    "        \n",
    "        combos = list(itertools.product(*param_names))\n",
    "        \n",
    "        for x in combos:\n",
    "            self.add_input_combo(*x)\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    def save_output(self, combo_id, data, model, backtest):\n",
    "        '''\n",
    "        save output of ML algorithm  in input_output dataframe \n",
    "        \n",
    "        inputs:\n",
    "            combo_id:int\n",
    "            data:strategydata object\n",
    "            model: predictor object\n",
    "            backtest backtest object\n",
    "        '''\n",
    "        \n",
    "        self.input_output_df.loc[combo_id,'trainable_parameters'] = model.model.count_params()\n",
    "        self.input_output_df.loc[combo_id,'training_records'] = len(data.features_train.index)\n",
    "        self.input_output_df.loc[combo_id,'precision'] = np.mean(model.results.history['precision'][-5:])\n",
    "        self.input_output_df.loc[combo_id,'accuracy'] = np.mean(model.results.history['accuracy'][-5:])\n",
    "        self.input_output_df.loc[combo_id,'recall'] = np.mean(model.results.history['recall'][-5:])\n",
    "        self.input_output_df.loc[combo_id,'gini'] = model.train_gini\n",
    "        self.input_output_df.loc[combo_id,'test_gini'] = model.test_gini\n",
    "        self.input_output_df.loc[combo_id,'test_accuracy'] = model.test_accuracy\n",
    "        \n",
    "        if self.input_output_df.loc[combo_id]['ml_type'] == 'DNN':\n",
    "            self.input_output_df.loc[combo_id,'val_precision'] = np.mean(model.results.history['val_precision'][-5:])\n",
    "            self.input_output_df.loc[combo_id,'val_accuracy'] = np.mean(model.results.history['val_accuracy'][-5:])\n",
    "            self.input_output_df.loc[combo_id,'val_recall'] = np.mean(model.results.history['val_recall'][-5:])\n",
    "            \n",
    "        if self.input_output_df.loc[combo_id]['long'] == True:\n",
    "            if data.labels_train['long_label'].value_counts().size == 2:\n",
    "                self.input_output_df.loc[combo_id,'train_label_freq'] = (data.labels_train['long_label'].value_counts()[1] \n",
    "                                                                 / data.labels_train['long_label'].size)\n",
    "            if data.labels_test['long_label'].value_counts().size == 2:\n",
    "                self.input_output_df.loc[combo_id,'test_label_freq'] = (data.labels_test['long_label'].value_counts()[1] \n",
    "                                                                    / data.labels_test['long_label'].size)\n",
    "            self.input_output_df.loc[combo_id,'gross_IR'] = backtest.gross_perf_metrics.loc['long']['IR']\n",
    "            self.input_output_df.loc[combo_id,'gross_IR_baseline'] = backtest.gross_perf_metrics.loc['baseline_long']['IR']\n",
    "            self.input_output_df.loc[combo_id,'net_IR'] = backtest.net_perf_metrics.loc['long']['IR']\n",
    "            self.input_output_df.loc[combo_id,'net_IR_baseline'] = backtest.net_perf_metrics.loc['baseline_long']['IR']\n",
    "        else:\n",
    "            if data.labels_train['short_label'].value_counts().size == 2:\n",
    "                self.input_output_df.loc[combo_id,'train_label_freq'] = (data.labels_train['short_label'].value_counts()[1] \n",
    "                                                                 / data.labels_train['short_label'].size)\n",
    "            if data.labels_test['short_label'].value_counts().size == 2:\n",
    "                self.input_output_df.loc[combo_id,'test_label_freq'] = (data.labels_test['short_label'].value_counts()[1] \n",
    "                                                                    / data.labels_test['short_label'].size)\n",
    "            \n",
    "            self.input_output_df.loc[combo_id,'gross_IR'] = backtest.gross_perf_metrics.loc['short']['IR']\n",
    "            self.input_output_df.loc[combo_id,'gross_IR_baseline'] = backtest.gross_perf_metrics.loc['baseline_short']['IR']\n",
    "            self.input_output_df.loc[combo_id,'net_IR'] = backtest.net_perf_metrics.loc['short']['IR']\n",
    "            self.input_output_df.loc[combo_id,'net_IR_baseline'] = backtest.net_perf_metrics.loc['baseline_short']['IR']\n",
    "    \n",
    "    def run(self,combo_id,es,vx,show_backtest_charts=False,model_fit_stats=False, save_output=True, \n",
    "            defined_features = None):\n",
    "        '''\n",
    "        calibrate ML model over one combination of inputs and store results  \n",
    "        \n",
    "        Inputs:\n",
    "            combo_id:int\n",
    "            es: instrument class\n",
    "                es data\n",
    "            vx: instrument class\n",
    "                vix data\n",
    "            show_backtest_charts: boolean\n",
    "                show strategy backtest metrics and equity charts\n",
    "            model_fit_stats: boolean\n",
    "                show model fit performance metrics (gini, roc, confusion matrix etc.)\n",
    "            save_output: boolean\n",
    "                save performance to input_output dataframe\n",
    "            defined_features:list\n",
    "                use user defined features \n",
    "            \n",
    "        \n",
    "        '''\n",
    "        # prepare data\n",
    "        _freq = self.input_output_df.loc[combo_id]['frequency']\n",
    "        _include_vix = vx if self.input_output_df.loc[combo_id]['vix']== True else None\n",
    "        \n",
    "        data = StrategyData(inst = es,freq = _freq,vix = _include_vix)\n",
    "        \n",
    "        data.create_features()\n",
    "        \n",
    "        _clip = self.input_output_df.loc[combo_id]['clip']\n",
    "        _date1 = self.input_output_df.loc[combo_id]['date1']\n",
    "        _date2 = self.input_output_df.loc[combo_id]['date2']\n",
    "        _date3 = self.input_output_df.loc[combo_id]['date3']\n",
    "\n",
    "        _target = self.input_output_df.loc[combo_id]['target_label']\n",
    "        _n = self.input_output_df.loc[combo_id]['n_label']\n",
    "        \n",
    "        # calculate standard deviation of \n",
    "        _t = data.inst.ticker\n",
    "        _training_d = data.cleansed_data.loc[(data.cleansed_data.index > _date1) & (data.cleansed_data.index <= _date2)]\n",
    "        _training_avg_c = _training_d['Close_'+_t].mean()\n",
    "        _training_std = (np.log(_training_d['Close_'+_t] / _training_d['Close_'+_t].shift(1))).std()\n",
    "        _target_std = _training_avg_c *  _training_std * _target * math.sqrt(_n)\n",
    "        \n",
    "        data.create_label(target_down=-1.*_target_std, target_up=_target_std,n=_n)\n",
    "        self.input_output_df.loc[combo_id,'target_up'] =  _target_std\n",
    "        self.input_output_df.loc[combo_id,'target_down'] = -1. * _target_std\n",
    "        \n",
    "        data.split_data_standardise([_date1,_date2,_date3],_clip)\n",
    "        \n",
    "        _start_trading_time = self.input_output_df.loc[combo_id]['start_trading_time']\n",
    "        _end_trading_time = self.input_output_df.loc[combo_id]['end_trading_time']\n",
    "        \n",
    "        data.set_trading_hours(start_time=_start_trading_time,end_time=_end_trading_time)\n",
    "        \n",
    "        #ML model\n",
    "        _ml_type = self.input_output_df.loc[combo_id]['ml_type'] \n",
    "        _epochs = self.input_output_df.loc[combo_id]['epochs']\n",
    "        _long = self.input_output_df.loc[combo_id]['long']\n",
    "        _hl = self.input_output_df.loc[combo_id]['hl']\n",
    "        _hu = self.input_output_df.loc[combo_id]['hu']\n",
    "        _dropout = self.input_output_df.loc[combo_id]['dropout']\n",
    "        _rate = self.input_output_df.loc[combo_id]['rate']\n",
    "        _run_boruta = self.input_output_df.loc[combo_id]['run_boruta']\n",
    "        _drop_correlated_features = self.input_output_df.loc[combo_id]['drop_correlated_features']\n",
    "        _optimiser = self.input_output_df.loc[combo_id]['optimiser']\n",
    "        _lr = self.input_output_df.loc[combo_id]['lr']\n",
    "        \n",
    "        if _ml_type == 'DNN':\n",
    "            _regularize = self.input_output_df.loc[combo_id]['regularize']\n",
    "            _reg_l1 = self.input_output_df.loc[combo_id]['reg_l1']\n",
    "\n",
    "            model = DNN(data, epochs=_epochs,hl=_hl,hu=_hu,\n",
    "                        dropout=_dropout,rate=_rate,long = _long,\n",
    "                        regularize=_regularize, reg_l1=_reg_l1, \n",
    "                        selected_features = defined_features)\n",
    "            \n",
    "            if defined_features is None:\n",
    "                if _run_boruta:\n",
    "                    model.boruta_fit(data)\n",
    "                    model.boruta_features_selection(data)\n",
    "                if _drop_correlated_features:\n",
    "                    model.drop_correlated_features(data)\n",
    "            \n",
    "            if _optimiser == 'sgd':\n",
    "                _momentum = self.input_output_df.loc[combo_id]['momentum']\n",
    "                _nesterov = self.input_output_df.loc[combo_id]['nesterov']\n",
    "                model.set_optimiser(name = 'sgd', lr=_lr, \n",
    "                                    momentum=_momentum, nesterov=_nesterov)\n",
    "                model.fit(data)\n",
    "                model.predict(data,show_charts=model_fit_stats)\n",
    "            else:\n",
    "                model.set_optimiser(name = _optimiser, lr=_lr)\n",
    "                model.fit(data)\n",
    "                model.predict(data,show_charts=model_fit_stats)\n",
    "        \n",
    "        elif _ml_type == 'DRNN':\n",
    "                _lags = self.input_output_df.loc[combo_id]['lags']\n",
    "                _layer = self.input_output_df.loc[combo_id]['layer']\n",
    "            \n",
    "                model = DRNN(data,lags=_lags,hl=_hl,hu=_hu,layer=_layer,\n",
    "                            dropout=_dropout,rate=_rate,epochs=_epochs,long=_long,\n",
    "                            selected_features=defined_features)\n",
    "                \n",
    "                if defined_features is None:\n",
    "                    if _run_boruta:\n",
    "                        model.boruta_fit(data)\n",
    "                        model.boruta_features_selection(data)\n",
    "                    if _drop_correlated_features:\n",
    "                        model.drop_correlated_features(data)\n",
    "                \n",
    "                if _optimiser == 'sgd':\n",
    "                    _momentum = self.input_output_df.loc[combo_id]['momentum']\n",
    "                    _nesterov = self.input_output_df.loc[combo_id]['nesterov']\n",
    "                    model.set_optimiser(name = 'sgd', lr=_lr, \n",
    "                                    momentum=_momentum, nesterov=_nesterov)\n",
    "                    model.fit(data,batch_size = 5,steps_per_epoch=10)\n",
    "                    model.predict(data,show_charts=model_fit_stats)\n",
    "                else:\n",
    "                    model.set_optimiser(name = _optimiser, lr=_lr)\n",
    "                    model.fit(data,batch_size = 5,steps_per_epoch=10)\n",
    "                    model.predict(data,show_charts=model_fit_stats)\n",
    "\n",
    "            \n",
    "        backtest = Strategy(data,model,model,show_chart=show_backtest_charts)\n",
    "        \n",
    "        if save_output:\n",
    "            self.save_output(combo_id, data, model, backtest)\n",
    "        \n",
    "        return data, model, backtest\n",
    "        \n",
    "    def run_long_short(self,combo_id_long,combo_id_short,es,vx,show_backtest_charts=True,\n",
    "                       defined_features_long = None, defined_features_short = None,\n",
    "                       model_fit_stats=True):\n",
    "        '''\n",
    "        run long and short ML model and then perform backtest\n",
    "        \n",
    "        Inputs:\n",
    "            combo_id_long:int\n",
    "                combo id of long approach\n",
    "            combo_id_short:int\n",
    "                combo id of short approach\n",
    "            es: instrument class\n",
    "                es data\n",
    "            vx: instrument class\n",
    "                vix data\n",
    "            show_backtest_charts: boolean\n",
    "                show performance statistics of backtest\n",
    "            defined_features_long:list\n",
    "                use user defined features for long model\n",
    "            defined_features_short:list\n",
    "                use user defined features for short model\n",
    "        '''\n",
    "\n",
    "        if self.input_output_df.loc[combo_id_long]['date1'] != self.input_output_df.loc[combo_id_short]['date1']:\n",
    "                print ('error : long and short sample not matching')\n",
    "                return\n",
    "            \n",
    "        if self.input_output_df.loc[combo_id_long]['date2'] != self.input_output_df.loc[combo_id_short]['date2']:\n",
    "                print ('error : long and short sample not matching')\n",
    "                return\n",
    "        \n",
    "        if self.input_output_df.loc[combo_id_long]['date3'] != self.input_output_df.loc[combo_id_short]['date3']:\n",
    "                print ('error : long and short sample not matching')\n",
    "                return\n",
    "\n",
    "        \n",
    "        \n",
    "        data,model_long,_  = self.run(combo_id_long,es,vx,model_fit_stats=model_fit_stats,save_output=False, \n",
    "                                      defined_features = defined_features_long)\n",
    "        \n",
    "        _,model_short,_ = self.run(combo_id_short,es,vx,model_fit_stats=model_fit_stats,save_output=False,\n",
    "                                      defined_features = defined_features_short)\n",
    "        \n",
    "        backtest = Strategy(data,model_long,model_short,show_chart=show_backtest_charts)\n",
    "        \n",
    "        return data, model_long, model_short, backtest\n",
    "\n",
    "    \n",
    "    def run_multiple(self,name,es,vx,defined_features=None, start_from_combo_id=None):\n",
    "        '''\n",
    "        run all combinations in the input_output dataframe and store results\n",
    "        \n",
    "        Inputs:\n",
    "            name:str\n",
    "                name of combinations\n",
    "            es: instrument object\n",
    "                es data\n",
    "            vx: instrument object\n",
    "                vix data\n",
    "            defined_features:list\n",
    "                use user defined features\n",
    "            start_from_combo_id: int\n",
    "                if not None, the loop will start from comboid\n",
    "        '''\n",
    "        for i in self.input_output_df.index:\n",
    "            if i<start_from_combo_id if start_from_combo_id is not None else 0:\n",
    "                continue\n",
    "            self.run(i,es,vx, save_output=True, defined_features=defined_features)\n",
    "            self.to_csv(name)\n",
    "            self.input_output_df.to_pickle(name)\n",
    "            print(str(i))\n",
    "        \n",
    "        winsound.Beep(440, 500)\n",
    "        \n",
    "    \n",
    "    def run_rolling(self,combo_id_long,combo_id_short,es,vx,months_backward,start_date=None):\n",
    "        '''\n",
    "        runs rolling ML models (one long and one short) that are recalibrated each month and \n",
    "        shows equity line\n",
    "        \n",
    "        Inputs:\n",
    "            combo_id_long:int\n",
    "                combo id of long approach\n",
    "            combo_id_short:int\n",
    "                combo id of short approach\n",
    "            es: instrument class\n",
    "                es data\n",
    "            vx: instrument class\n",
    "                vix data\n",
    "            months_backward: int\n",
    "                number of months for rolling window for calibration\n",
    "            start_date: datetime\n",
    "                user defined start date for rolling calibration\n",
    "        '''\n",
    "        #create months array for cross validation\n",
    "        \n",
    "        data = es.data if start_date == None else es.data[es.data.index>=start_date]\n",
    "        \n",
    "        start_date = data[:1].index.date[0]\n",
    "        end_date = data[-1:].index.date[0]\n",
    "        n_months = (end_date.year - start_date.year)*12 + (end_date.month - start_date.month) + 1 \n",
    "        months = pn.date_range(start=start_date, periods = n_months, freq='1M')\n",
    "        \n",
    "        #dataframe to store strategy returns and performance stats\n",
    "        strategy_gross_log_returns = pn.DataFrame()\n",
    "        strategy_log_returns = pn.DataFrame()\n",
    "        results = pn.DataFrame()\n",
    "        long_pred_probabilities = pn.DataFrame()\n",
    "        short_pred_probabilities = pn.DataFrame()\n",
    "        \n",
    "        #rolling cross validation\n",
    "        for m in range(months_backward+1,len(months)):\n",
    "            print(months[m])\n",
    "\n",
    "            #update long model dates\n",
    "            self.input_output_df.loc[combo_id_long,'date1'] = months[m - 1 - months_backward] \n",
    "            self.input_output_df.loc[combo_id_long,'date2'] = months[m - 1] \n",
    "            self.input_output_df.loc[combo_id_long,'date3'] = months[m]\n",
    "            \n",
    "            #update short model dates\n",
    "            self.input_output_df.loc[combo_id_short,'date1'] = months[m - 1 - months_backward] \n",
    "            self.input_output_df.loc[combo_id_short,'date2'] = months[m - 1] \n",
    "            self.input_output_df.loc[combo_id_short,'date3'] = months[m]\n",
    "            \n",
    "            _,model_long, model_short, backtest = self.run_long_short(combo_id_long,combo_id_short,\n",
    "                                                                      es,vx,show_backtest_charts=False,\n",
    "                                                                      model_fit_stats=False)\n",
    "            \n",
    "            long_pred_probabilities = pn.concat([long_pred_probabilities, model_long.test_predict])\n",
    "            short_pred_probabilities = pn.concat([short_pred_probabilities, model_short.test_predict])\n",
    "            \n",
    "            strategy_gross_log_returns = pn.concat([strategy_gross_log_returns, backtest.gross_log_returns])\n",
    "            strategy_log_returns = pn.concat([strategy_log_returns, backtest.Net_log_returns])\n",
    "            \n",
    "            results.loc[months[m],'test_gini_long'] = model_long.test_gini\n",
    "            results.loc[months[m],'test_gini_short'] = model_short.test_gini\n",
    "            results.loc[months[m],'train_gini_long'] = model_long.train_gini\n",
    "            results.loc[months[m],'train_gini_short'] = model_short.train_gini\n",
    "            \n",
    "            results.loc[months[m],'test_accuracy_long'] = model_long.test_accuracy\n",
    "            results.loc[months[m],'test_accuracy_short'] = model_short.test_accuracy\n",
    "            results.loc[months[m],'train_accuracy_long'] = model_long.train_accuracy\n",
    "            results.loc[months[m],'train_accuracy_short'] = model_short.train_accuracy\n",
    "            \n",
    "            results.loc[months[m],'test_precision_long'] = model_long.test_precision\n",
    "            results.loc[months[m],'test_precision_short'] = model_short.test_precision\n",
    "            results.loc[months[m],'train_precision_long'] = model_long.train_precision\n",
    "            results.loc[months[m],'train_precision_short'] = model_short.train_precision\n",
    "            \n",
    "            results.loc[months[m],'test_precision_recall'] = model_long.test_recall\n",
    "            results.loc[months[m],'test_precision_recall'] = model_short.test_recall\n",
    "            results.loc[months[m],'train_precision_recall'] = model_long.train_recall\n",
    "            results.loc[months[m],'train_precision_recall'] = model_short.train_recall\n",
    "        \n",
    "        strategy_gross_log_returns.dropna(inplace=True)\n",
    "        strategy_log_returns.dropna(inplace=True)\n",
    "        \n",
    "        perf_stats_net = backtest.calculate_perf_metrics(strategy_log_returns,backtest.max_bars_per_day)\n",
    "        \n",
    "        strategy_log_returns.cumsum().apply(np.exp).plot(figsize=(16, 6),title='net strategy returns');\n",
    "        print(perf_stats_net) \n",
    "        \n",
    "        winsound.Beep(440, 500)\n",
    "        \n",
    "        self.rolling_perf_stats_net = perf_stats_net\n",
    "        self.rolling_strategy_gross_log_returns = strategy_gross_log_returns\n",
    "        self.rolling_strategy_log_returns = strategy_log_returns\n",
    "        self.rolling_long_pred_probabilities = long_pred_probabilities\n",
    "        self.rolling_short_pred_probabilities = short_pred_probabilities\n",
    "        \n",
    "        return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
